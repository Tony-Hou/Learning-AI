## [2021.06.20 CVPR 2021](https://www.youtube.com/watch?v=g6bOwQdCJrc)
- The grand mission: Tesla is ditching radars. They are using neural network and vision to do radar depth + velocity sensing.
- In order to do that, they need a large AND diverse 4D (3D+time) dataset. This is also used to train FSD. 
- Tesla has a whole team spending about 4 months focusing on autolabeling 
- Tesla uses MANY (221 as of mid-2021) triggers to collect the diverse dataset. They ended up with 1 million 10-second clips.
- Dedicated HPC team. Now Tesla training with 720 8-GPU nodes!
- Tesla argues that vision alone is perfectly capable of depth sensing. It is hard and it requires the fleet.
![](assets/cover.jpg)




PMM: pedal misuse mitigation
![](assets/traffic_control_warning_pmm.jpg)

Tesla's data set-up.
![](assets/tesla_no_radar.jpg)
![](assets/8cam_setup.jpg)
![](assets/large_clean_diverse_data.jpg)

Have to figure out the road layout the first time the car goes there (drive on perception). Fundamental problem: Depth estimation of monocular 
![](assets/data_auto_labeling.jpg)
![](assets/trainig_cluster.jpg)
![](assets/tesla_dataset.jpg)

Once in a while radar gives you a FP that is hard to handle
![](assets/depth_velocity_with_vision_1.jpg)
![](assets/depth_velocity_with_vision_2.jpg)
![](assets/depth_velocity_with_vision_3.jpg)

Validation process
![](assets/release_and_validation.jpg)
